{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import argparse\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "from dataprepaug import *\n",
    "from utils import *\n",
    "from models import *\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "#from models.unet_model import UNet\n",
    "#from models.UnetPlusPlus import NestedUNet\n",
    "#from .pspnet import *\n",
    "#from .deeplab import *\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numbers\n",
    "import math\n",
    "import PIL\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from optuna import Trial\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding: utf-8\n",
    "\n",
    "# In[6]:\n",
    "def train(traindataload,net,optimizer, grad_sc,epoch,device):\n",
    "        \n",
    "        loss = 0\n",
    "        epoch_loss = 0\n",
    "        size=0\n",
    "        acc=0\n",
    "        nbatch = len(traindataload)\n",
    "        for idx, (inputs, targets) in enumerate(traindataload):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = net(inputs)\n",
    "                size += outputs.shape[0]*outputs.shape[2]*outputs.shape[3]\n",
    "                batch_loss = dice_loss(outputs.softmax(1),targets, multiclass=True)\n",
    "                optimizer.zero_grad()\n",
    "                grad_sc.scale(batch_loss).backward()\n",
    "                grad_sc.step(optimizer)\n",
    "                grad_sc.update()\n",
    "                loss = batch_loss.item()\n",
    "                epoch_loss += loss\n",
    "                acc += (outputs.argmax(1) == targets.argmax(1)).type(torch.float).sum().item()\n",
    "                #progress_bar(idx, len(traindataload), 'Loss: %.5f, Dice-Coef: %.5f'\n",
    "                #         %(loss, 1-loss))#(loss/(idx+1)), (1-(loss/(idx+1)))))\n",
    "                log_msg = '\\n'.join(['Training (%d/%d): Epoch: %d, Loss: %.5f,  Dice-Coef:  %.5f' %(idx,nbatch, epoch,loss, 1-loss)])\n",
    "                logging.info(log_msg)\n",
    "        epoch_loss /= nbatch\n",
    "        acc /= size\n",
    "        print(f\"Training epoch error: \\n Acc: {(100*acc):>0.1f}%, Avg loss: {epoch_loss:>8f}, Dice: {(1-epoch_loss):>8f} \\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "def validation(validationload,net,epoch,device):\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        size=0\n",
    "        nbatch = len(validationload)\n",
    "        with torch.no_grad():\n",
    "                for idx, (inputs, targets) in enumerate(validationload):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "                        outputs = net(inputs)\n",
    "                        loss += dice_loss(outputs.softmax(1),targets, multiclass=True).item()\n",
    "                        acc += (outputs.argmax(1) == targets.argmax(1)).type(torch.float).sum().item()\n",
    "                        size += outputs.shape[0]*outputs.shape[2]*outputs.shape[3]\n",
    "            \n",
    "                        \n",
    "                        #progress_bar(idx, len(validationload), 'Loss: %.5f, Dice-Coef: %.5f' %(loss/(idx+1), 1-(loss/(idx+1))))\n",
    "                        log_msg = '\\n'.join(['Validation (%d/%d): Epoch: %d  Loss: %.5f,  Dice-Coef:  %.5f'\n",
    "                %(idx, nbatch, epoch, loss/(idx+1), 1-(loss/(idx+1)))])\n",
    "                        logging.info(log_msg)\n",
    "        \n",
    "        loss /= nbatch\n",
    "        acc /= size\n",
    "                        \n",
    "        print(f\"Validation error: \\n Acc: {(100*acc):>0.1f}%, Avg loss: {loss:>8f}, Dice: {(1-loss):>8f} \\n\")\n",
    "        return loss, acc\n",
    "                                \n",
    "def trainmodel(net, optimizer, checkpoint_dir=None):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "    cudnn.benchmark = True\n",
    "    get_logger()\n",
    "    \n",
    "    file_path_train = \"/home/jay/Documents/courses/Aicourse/Brats/HYPER/train/\"\n",
    "    \n",
    "    \n",
    "    file_path_validation = \"/home/jay/Documents/courses/Aicourse/Brats/HYPER/validation/\"\n",
    "    print('loading Data')\n",
    "    #net=NestedUNet(4,4, layer_multipler= config[\"layer_multipler\"])   #load_model(args, class_num=4, mode='train')\n",
    "    #optimizer = SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9, weight_decay=1e-8)\n",
    "        \n",
    "    net=net.to(device)\n",
    "    \n",
    "        \n",
    "    traindataload = data_load(file_path_train, batch_size = config[\"batch_size\"],datatype='Train')\n",
    "    validationload = data_load(file_path_validation,batch_size = config[\"batch_size\"],datatype = 'Validation')\n",
    "    \n",
    "    grad_sc = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    \n",
    "        \n",
    "    ckpt_root = \"/home/jay/Documents/courses/Aicourse/Brats/checkpointUnetPlusPlusHYPER/\"\n",
    "    train(traindataload,net,optimizer, grad_sc,epoch,device)\n",
    "    print('\\n\\n<Validation>')\n",
    "    [validloss, acc] = validation(validationload,net,epoch,device)\n",
    "       \n",
    "        # Save Model\n",
    "    try:\n",
    "            os.makedirs(ckpt_root)\n",
    "    except:\n",
    "            pass\n",
    "        #loss /= (idx+1)\n",
    "        #score = 1 - validloss\n",
    "        #if score > best_score:\n",
    "    checkpoint = Checkpoint(net, optimizer, epoch, score)\n",
    "    checkpoint.save(os.path.join(args.ckpt_root, args.model+'.tar'))\n",
    "    best_score = score\n",
    "    print(\"Saving...\")\n",
    "        \n",
    "        \n",
    "    \n",
    "def optumaParams(trial):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        config = {\n",
    "        \"layer_multipler\": trial.suggest_discrete_uniform(\"layer_multipler\", 1,3,1),\n",
    "        \"lr\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-1),\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\",4,16,2),\n",
    "        \"optimizer\": trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"])\n",
    "    }\n",
    "\n",
    "        \n",
    "        net = NestedUNet(4,4, layer_multipler= config[\"layer_multipler\"])\n",
    "        net = net.to(device)    \n",
    "        optimizer = getattr(optim, config[\"optimizer\"])(net.parameters(), lr=config[\"lr\"])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  \n",
    "        \n",
    "        \n",
    "        file_path_train = \"/home/jay/Documents/courses/Aicourse/Brats/HYPER/train/\"\n",
    "    \n",
    "    \n",
    "        file_path_validation = \"/home/jay/Documents/courses/Aicourse/Brats/HYPER/validation/\"\n",
    "        print('loading Data')\n",
    "    #net=NestedUNet(4,4, layer_multipler= config[\"layer_multipler\"])   #load_model(args, class_num=4, mode='train')\n",
    "    #optimizer = SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9, weight_decay=1e-8)\n",
    "        \n",
    "    \n",
    "        \n",
    "        traindataload = data_load(file_path_train, batch_size = config[\"batch_size\"],datatype='Train')\n",
    "        validationload = data_load(file_path_validation,batch_size = config[\"batch_size\"],datatype = 'Validation')\n",
    "    \n",
    "        grad_sc = torch.cuda.amp.GradScaler(enabled=True)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "        # Train Model\n",
    "                print(f'Epoch: {epoch}')\n",
    "                train(traindataload,net,optimizer, grad_sc,epoch,device)\n",
    "                print('\\n\\n<Validation>')\n",
    "                [validloss, acc] = validation(validationload,net,epoch,device)\n",
    "                trial.report(validloss,epoch)\n",
    "        \n",
    "        model_save = 'model_save_trial' + str(trial.number) + '.pth'\n",
    "        torch.save(net.state_dict(), model_save)\n",
    "        return validloss   \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-15 10:58:42,393]\u001b[0m A new study created in memory with name: no-name-e2c1c8f8-bb5f-482b-a599-0681e0c37ccf\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Data\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jay/.local/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch error: \n",
      " Acc: 46.4%, Avg loss: 0.885962, Dice: 0.114038 \n",
      "\n",
      "\n",
      "\n",
      "<Validation>\n",
      "Validation error: \n",
      " Acc: 72.5%, Avg loss: 0.888672, Dice: 0.111328 \n",
      "\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    epochs = 5\n",
    "    studyOptuma = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler())\n",
    "    studyOptuma.optimize(optumaParams, n_trials=40, gc_after_trial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
