{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import torch\n",
    "import glob\n",
    "import os, sys, time\n",
    "import h5py\n",
    "import numbers\n",
    "import random\n",
    "from torch.utils import data\n",
    "import helpers\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(args, datatype):\n",
    "    \n",
    "    if datatype=='Train':\n",
    "        data = TrainData(args)\n",
    "        shuffle=True\n",
    "        \n",
    "    elif datatype=='Validation':\n",
    "        data = ValidData(args)\n",
    "\n",
    "    elif datatype=='Test':\n",
    "        data = TestData(args)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Choose the data type: Train, Validation, Test')\n",
    "        \n",
    "        \n",
    "    dataload =  torch.utils.data.DataLoader(data, batch_size=args.batch_size, \n",
    "                                            shuffle=shuffle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(torch.utils.data.Dataset):\n",
    "     \n",
    "    def __init__(self, file_path, data_cache_size=3, transform=None):\n",
    "        recursive=False \n",
    "        load_data=False\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "        self.transform = transform #Compose([\n",
    "                    #RandomVerticalFlip(),\n",
    "                    #RandomHorizontalFlip(),\n",
    "                    #RandomAffine(degrees=(-20,20),translate=(0.1,0.1),\n",
    "                    #             scale=(0.9,1.1), shear=(-0.2,0.2))])\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        assert(p.is_dir())\n",
    "        if recursive:\n",
    "            files = sorted(p.glob('**/*.h5'))\n",
    "        else:\n",
    "            files = sorted(p.glob('*.h5'))\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"image\", index)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        else:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"mask\", index)\n",
    "        y = torch.from_numpy(y)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('image'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path, load_data):\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            # Walk through all groups, extracting datasets\n",
    "        \n",
    "                for dname, ds in h5_file.items():\n",
    "                    # if data is not loaded its cache index is -1\n",
    "                    idx = -1\n",
    "                    if load_data:\n",
    "                        # add data to the data cache\n",
    "                        \n",
    "                        if dname == 'mask':\n",
    "                            mask = np.zeros(np.shape(ds[()]))\n",
    "                            mask[:,:,0] = ds[()][:,:,0]*50\n",
    "                            mask[:,:,1] = ds[()][:,:,1]*100\n",
    "                            mask[:,:,2] = ds[()][:,:,2]*150\n",
    "                            maskflat = mask[:,:,0]+mask[:,:,1]+mask[:,:,2]\n",
    "                            idex = self._add_to_cache(maskflat, file_path)\n",
    "                            \n",
    "                        else:\n",
    "                            image = np.zeros(np.shape(ds[()]))\n",
    "                            image[:,:,0] = cv2.normalize(ds[()][:,:,0],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,1] = cv2.normalize(ds[()][:,:,1],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,2] = cv2.normalize(ds[()][:,:,2],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,3] = cv2.normalize(ds[()][:,:,3],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            idx = self._add_to_cache(image, file_path)\n",
    "                    \n",
    "                    # type is derived from the name of the dataset; we expect the dataset\n",
    "                    # name to have a name such as 'data' or 'label' to identify its type\n",
    "                    # we also store the shape of the data in case we need it\n",
    "                    self.data_info.append({'file_path': file_path, 'type': dname, 'shape': np.shape(ds[()]), 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            \n",
    "                for dname, ds in h5_file.items():\n",
    "                    # add data to the data cache and retrieve\n",
    "                    # the cache index\n",
    "                    if dname == 'mask':\n",
    "                            mask = np.zeros(np.shape(ds[()]))\n",
    "                            mask[:,:,0] = ds[()][:,:,0]*50\n",
    "                            mask[:,:,1] = ds[()][:,:,1]*100\n",
    "                            mask[:,:,2] = ds[()][:,:,2]*150\n",
    "                            maskflat = mask[:,:,0]+mask[:,:,1]+mask[:,:,2]\n",
    "                            idex = self._add_to_cache(maskflat, file_path)\n",
    "                            \n",
    "                    else:\n",
    "                            image = np.zeros(np.shape(ds[()]))\n",
    "                            image[:,:,0] = cv2.normalize(ds[()][:,:,0],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,1] = cv2.normalize(ds[()][:,:,1],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,2] = cv2.normalize(ds[()][:,:,2],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,3] = cv2.normalize(ds[()][:,:,3],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            idx = self._add_to_cache(image, file_path)\n",
    "                            \n",
    "                    # find the beginning index of the hdf5 file we are looking for\n",
    "                    file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                    # the data info should have the same index since we loaded it in the same way\n",
    "                    self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            \n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "        #print(file_path)\n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidData(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, file_path, data_cache_size=3, transform=None):\n",
    "        recursive=False \n",
    "        load_data=False\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        assert(p.is_dir())\n",
    "        if recursive:\n",
    "            files = sorted(p.glob('**/*.h5'))\n",
    "        else:\n",
    "            files = sorted(p.glob('*.h5'))\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"image\", index)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        else:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"mask\", index)\n",
    "        y = torch.from_numpy(y)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('image'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path, load_data):\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            # Walk through all groups, extracting datasets\n",
    "        \n",
    "                for dname, ds in h5_file.items():\n",
    "                    # if data is not loaded its cache index is -1\n",
    "                    idx = -1\n",
    "                    if load_data:\n",
    "                        # add data to the data cache\n",
    "                        \n",
    "                        if dname == 'mask':\n",
    "                            mask = np.zeros(np.shape(ds[()]))\n",
    "                            mask[:,:,0] = ds[()][:,:,0]*50\n",
    "                            mask[:,:,1] = ds[()][:,:,1]*100\n",
    "                            mask[:,:,2] = ds[()][:,:,2]*150\n",
    "                            maskflat = mask[:,:,0]+mask[:,:,1]+mask[:,:,2]\n",
    "                            idex = self._add_to_cache(maskflat, file_path)\n",
    "                            \n",
    "                        else:\n",
    "                            image = np.zeros(np.shape(ds[()]))\n",
    "                            image[:,:,0] = cv2.normalize(ds[()][:,:,0],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,1] = cv2.normalize(ds[()][:,:,1],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,2] = cv2.normalize(ds[()][:,:,2],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,3] = cv2.normalize(ds[()][:,:,3],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            idx = self._add_to_cache(image, file_path)\n",
    "                    \n",
    "                    # type is derived from the name of the dataset; we expect the dataset\n",
    "                    # name to have a name such as 'data' or 'label' to identify its type\n",
    "                    # we also store the shape of the data in case we need it\n",
    "                    self.data_info.append({'file_path': file_path, 'type': dname, 'shape': np.shape(ds[()]), 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            \n",
    "                for dname, ds in h5_file.items():\n",
    "                    # add data to the data cache and retrieve\n",
    "                    # the cache index\n",
    "                    if dname == 'mask':\n",
    "                            mask = np.zeros(np.shape(ds[()]))\n",
    "                            mask[:,:,0] = ds[()][:,:,0]*50\n",
    "                            mask[:,:,1] = ds[()][:,:,1]*100\n",
    "                            mask[:,:,2] = ds[()][:,:,2]*150\n",
    "                            maskflat = mask[:,:,0]+mask[:,:,1]+mask[:,:,2]\n",
    "                            idex = self._add_to_cache(maskflat, file_path)\n",
    "                            \n",
    "                    else:\n",
    "                            image = np.zeros(np.shape(ds[()]))\n",
    "                            image[:,:,0] = cv2.normalize(ds[()][:,:,0],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,1] = cv2.normalize(ds[()][:,:,1],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,2] = cv2.normalize(ds[()][:,:,2],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,3] = cv2.normalize(ds[()][:,:,3],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            idx = self._add_to_cache(image, file_path)\n",
    "                            \n",
    "                    # find the beginning index of the hdf5 file we are looking for\n",
    "                    file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                    # the data info should have the same index since we loaded it in the same way\n",
    "                    self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            \n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "        #print(file_path)\n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestData(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, data_cache_size=3, transform=None):\n",
    "        recursive=False \n",
    "        load_data=False\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "        self.transform = transform\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        assert(p.is_dir())\n",
    "        if recursive:\n",
    "            files = sorted(p.glob('**/*.h5'))\n",
    "        else:\n",
    "            files = sorted(p.glob('*.h5'))\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"image\", index)\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        else:\n",
    "            x = torch.from_numpy(x)\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"mask\", index)\n",
    "        y = torch.from_numpy(y)\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('image'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path, load_data):\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            # Walk through all groups, extracting datasets\n",
    "        \n",
    "                for dname, ds in h5_file.items():\n",
    "                    # if data is not loaded its cache index is -1\n",
    "                    idx = -1\n",
    "                    if load_data:\n",
    "                        # add data to the data cache\n",
    "                        \n",
    "                        if dname == 'mask':\n",
    "                            mask = np.zeros(np.shape(ds[()]))\n",
    "                            mask[:,:,0] = ds[()][:,:,0]*50\n",
    "                            mask[:,:,1] = ds[()][:,:,1]*100\n",
    "                            mask[:,:,2] = ds[()][:,:,2]*150\n",
    "                            maskflat = mask[:,:,0]+mask[:,:,1]+mask[:,:,2]\n",
    "                            idex = self._add_to_cache(maskflat, file_path)\n",
    "                            \n",
    "                        else:\n",
    "                            image = np.zeros(np.shape(ds[()]))\n",
    "                            image[:,:,0] = cv2.normalize(ds[()][:,:,0],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,1] = cv2.normalize(ds[()][:,:,1],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,2] = cv2.normalize(ds[()][:,:,2],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,3] = cv2.normalize(ds[()][:,:,3],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            idx = self._add_to_cache(image, file_path)\n",
    "                    \n",
    "                    # type is derived from the name of the dataset; we expect the dataset\n",
    "                    # name to have a name such as 'data' or 'label' to identify its type\n",
    "                    # we also store the shape of the data in case we need it\n",
    "                    self.data_info.append({'file_path': file_path, 'type': dname, 'shape': np.shape(ds[()]), 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path):\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path) as h5_file:\n",
    "            \n",
    "                for dname, ds in h5_file.items():\n",
    "                    # add data to the data cache and retrieve\n",
    "                    # the cache index\n",
    "                    if dname == 'mask':\n",
    "                            mask = np.zeros(np.shape(ds[()]))\n",
    "                            mask[:,:,0] = ds[()][:,:,0]*50\n",
    "                            mask[:,:,1] = ds[()][:,:,1]*100\n",
    "                            mask[:,:,2] = ds[()][:,:,2]*150\n",
    "                            maskflat = mask[:,:,0]+mask[:,:,1]+mask[:,:,2]\n",
    "                            idex = self._add_to_cache(maskflat, file_path)\n",
    "                            \n",
    "                    else:\n",
    "                            image = np.zeros(np.shape(ds[()]))\n",
    "                            image[:,:,0] = cv2.normalize(ds[()][:,:,0],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,1] = cv2.normalize(ds[()][:,:,1],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,2] = cv2.normalize(ds[()][:,:,2],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            image[:,:,3] = cv2.normalize(ds[()][:,:,3],None, norm_type=cv2.NORM_MINMAX)\n",
    "                            idx = self._add_to_cache(image, file_path)\n",
    "                            \n",
    "                    # find the beginning index of the hdf5 file we are looking for\n",
    "                    file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                    # the data info should have the same index since we loaded it in the same way\n",
    "                    self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            \n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "        #print(file_path)\n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#                                                           #\n",
    "#       Data Transforms Functions                           # \n",
    "#                                                           #\n",
    "#############################################################\n",
    "\n",
    "'''\n",
    "    From torchvision Transforms.py (+ Slightly changed)\n",
    "    (https://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py)\n",
    "'''\n",
    "\n",
    "class Compose(object):\n",
    "    \"\"\"Composes several transforms together.\n",
    "    Args:\n",
    "        transforms (list of ``Transform`` objects): list of transforms to compose.\n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.CenterCrop(10),\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        for t in self.transforms:\n",
    "            img,label = t(img, label)\n",
    "        return img, label\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '('\n",
    "        for t in self.transforms:\n",
    "            format_string += '\\n'\n",
    "            format_string += '    {0}'.format(t)\n",
    "        format_string += '\\n)'\n",
    "        return format_string\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
    "    Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
    "    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        return to_tensor(pic), to_tensor(label)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '()'\n",
    "\n",
    "\n",
    "class RandomVerticalFlip(object):\n",
    "    \"\"\"Vertically flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return vflip(img), vflip(label)\n",
    "        return img, label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    \"\"\"Horizontally flip the given PIL Image randomly with a given probability.\n",
    "    Args:\n",
    "        p (float): probability of the image being flipped. Default value is 0.5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be flipped.\n",
    "        Returns:\n",
    "            PIL Image: Randomly flipped image.\n",
    "        \"\"\"\n",
    "        if random.random() < self.p:\n",
    "            return hflip(img), hflip(label)\n",
    "        return img, label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(p={})'.format(self.p)\n",
    "\n",
    "\n",
    "class RandomRotation(object):\n",
    "    \"\"\"Rotate the image by angle.\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees).\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter. See `filters`_ for more information.\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        expand (bool, optional): Optional expansion flag.\n",
    "            If true, expands the output to make it large enough to hold the entire rotated image.\n",
    "            If false or omitted, make the output image the same size as the input image.\n",
    "            Note that the expand flag assumes rotation around the center and no translation.\n",
    "        center (2-tuple, optional): Optional center of rotation.\n",
    "            Origin is the upper left corner.\n",
    "            Default is the center of the image.\n",
    "    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees=360, resample=False, expand=False, center=None):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            if len(degrees) != 2:\n",
    "                raise ValueError(\"If degrees is a sequence, it must be of len 2.\")\n",
    "            self.degrees = degrees\n",
    "\n",
    "        self.resample = resample\n",
    "        self.expand = expand\n",
    "        self.center = center\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees):\n",
    "        \"\"\"Get parameters for ``rotate`` for a random rotation.\n",
    "        Returns:\n",
    "            sequence: params to be passed to ``rotate`` for random rotation.\n",
    "        \"\"\"\n",
    "        #angle = random.uniform(degrees[0], degrees[1])\n",
    "        angle_list = [0,90,180,270]\n",
    "        angle = random.choice(angle_list)\n",
    "        return angle\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be rotated.\n",
    "        Returns:\n",
    "            PIL Image: Rotated image.\n",
    "        \"\"\"\n",
    "\n",
    "        #angle = self.get_params(self.degrees)\n",
    "        angle = np.random.randint(self.degrees[0], self.degrees[1])\n",
    "        return rotate(img, angle, self.resample, self.expand, self.center),\\\n",
    "                rotate(label, angle, self.resample, self.expand, self.center)\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + '(degrees={0}'.format(self.degrees)\n",
    "        format_string += ', resample={0}'.format(self.resample)\n",
    "        format_string += ', expand={0}'.format(self.expand)\n",
    "        if self.center is not None:\n",
    "            format_string += ', center={0}'.format(self.center)\n",
    "        format_string += ')'\n",
    "        return format_string\n",
    "\n",
    "\n",
    "class RandomAffine(object):\n",
    "    \"\"\"Random affine transformation of the image keeping center invariant\n",
    "    Args:\n",
    "        degrees (sequence or float or int): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees). Set to 0 to deactivate rotations.\n",
    "        translate (tuple, optional): tuple of maximum absolute fraction for horizontal\n",
    "            and vertical translations. For example translate=(a, b), then horizontal shift\n",
    "            is randomly sampled in the range -img_width * a < dx < img_width * a and vertical shift is\n",
    "            randomly sampled in the range -img_height * b < dy < img_height * b. Will not translate by default.\n",
    "        scale (tuple, optional): scaling factor interval, e.g (a, b), then scale is\n",
    "            randomly sampled from the range a <= scale <= b. Will keep original scale by default.\n",
    "        shear (sequence or float or int, optional): Range of degrees to select from.\n",
    "            If degrees is a number instead of sequence like (min, max), the range of degrees\n",
    "            will be (-degrees, +degrees). Will not apply shear by default\n",
    "        resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional):\n",
    "            An optional resampling filter. See `filters`_ for more information.\n",
    "            If omitted, or if the image has mode \"1\" or \"P\", it is set to PIL.Image.NEAREST.\n",
    "        fillcolor (int): Optional fill color for the area outside the transform in the output image. (Pillow>=5.0.0)\n",
    "    .. _filters: https://pillow.readthedocs.io/en/latest/handbook/concepts.html#filters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, degrees, translate=None, scale=None, shear=None, resample=False, fillcolor=0):\n",
    "        if isinstance(degrees, numbers.Number):\n",
    "            if degrees < 0:\n",
    "                raise ValueError(\"If degrees is a single number, it must be positive.\")\n",
    "            self.degrees = (-degrees, degrees)\n",
    "        else:\n",
    "            assert isinstance(degrees, (tuple, list)) and len(degrees) == 2, \\\n",
    "                \"degrees should be a list or tuple and it must be of length 2.\"\n",
    "            self.degrees = degrees\n",
    "\n",
    "        if translate is not None:\n",
    "            assert isinstance(translate, (tuple, list)) and len(translate) == 2, \\\n",
    "                \"translate should be a list or tuple and it must be of length 2.\"\n",
    "            for t in translate:\n",
    "                if not (0.0 <= t <= 1.0):\n",
    "                    raise ValueError(\"translation values should be between 0 and 1\")\n",
    "        self.translate = translate\n",
    "\n",
    "        if scale is not None:\n",
    "            assert isinstance(scale, (tuple, list)) and len(scale) == 2, \\\n",
    "                \"scale should be a list or tuple and it must be of length 2.\"\n",
    "            for s in scale:\n",
    "                if s <= 0:\n",
    "                    raise ValueError(\"scale values should be positive\")\n",
    "        self.scale = scale\n",
    "\n",
    "        if shear is not None:\n",
    "            if isinstance(shear, numbers.Number):\n",
    "                if shear < 0:\n",
    "                    raise ValueError(\"If shear is a single number, it must be positive.\")\n",
    "                self.shear = (-shear, shear)\n",
    "            else:\n",
    "                assert isinstance(shear, (tuple, list)) and len(shear) == 2, \\\n",
    "                    \"shear should be a list or tuple and it must be of length 2.\"\n",
    "                self.shear = shear\n",
    "        else:\n",
    "            self.shear = shear\n",
    "\n",
    "        self.resample = resample\n",
    "        self.fillcolor = fillcolor\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(degrees, translate, scale_ranges, shears, img_size):\n",
    "        \"\"\"Get parameters for affine transformation\n",
    "        Returns:\n",
    "            sequence: params to be passed to the affine transformation\n",
    "        \"\"\"\n",
    "        angle = random.uniform(degrees[0], degrees[1])\n",
    "        if translate is not None:\n",
    "            max_dx = translate[0] * img_size[0]\n",
    "            max_dy = translate[1] * img_size[1]\n",
    "            translations = (np.round(random.uniform(-max_dx, max_dx)),\n",
    "                            np.round(random.uniform(-max_dy, max_dy)))\n",
    "        else:\n",
    "            translations = (0, 0)\n",
    "\n",
    "        if scale_ranges is not None:\n",
    "            scale = random.uniform(scale_ranges[0], scale_ranges[1])\n",
    "        else:\n",
    "            scale = 1.0\n",
    "\n",
    "        if shears is not None:\n",
    "            shear = random.uniform(shears[0], shears[1])\n",
    "        else:\n",
    "            shear = 0.0\n",
    "\n",
    "        return angle, translations, scale, shear\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        \"\"\"\n",
    "            img (PIL Image): Image to be transformed.\n",
    "        Returns:\n",
    "            PIL Image: Affine transformed image.\n",
    "        \"\"\"\n",
    "        ret = self.get_params(self.degrees, self.translate, self.scale, self.shear, img.size)\n",
    "        return affine(img, label, *ret, resample=self.resample, fillcolor=self.fillcolor)\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = '{name}(degrees={degrees}'\n",
    "        if self.translate is not None:\n",
    "            s += ', translate={translate}'\n",
    "        if self.scale is not None:\n",
    "            s += ', scale={scale}'\n",
    "        if self.shear is not None:\n",
    "            s += ', shear={shear}'\n",
    "        if self.resample > 0:\n",
    "            s += ', resample={resample}'\n",
    "        if self.fillcolor != 0:\n",
    "            s += ', fillcolor={fillcolor}'\n",
    "        s += ')'\n",
    "        d = dict(self.__dict__)\n",
    "        d['resample'] = _pil_interpolation_to_str[d['resample']]\n",
    "        return s.format(name=self.__class__.__name__, **d)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
